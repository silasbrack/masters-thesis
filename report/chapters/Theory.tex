\chapter{Theory}

\section{Optimization in Deep Learning}

Suppose a neural network is a real-valued function \(f: \reals^n \times \reals^d \rightarrow \reals^o\) parameterized in \(\bm\theta\) which maps an input \(\bm x\) to an output \(f(\bm x; \bm\theta) \equiv f_{\bm\theta}(\bm x)\).
Our goal is to find the optimal parameters \(\bm\theta^*\) which best model the observed data.

From a frequentist perspective, we can define a loss function \(\mathcal{L}(\bm\theta): \reals^d \rightarrow \reals\) such that the optimal parameters \(\bm\theta^*\) minimize this loss function.
Often the loss function is defined as the negative log-likelihood of the data under the model, \(\mathcal{L}(\bm\theta) = -\log \lik\).
In this case, the goal of optimization is to find the parameters \(\bm\theta^*\) which maximize the likelihood of the data~\footnote{Maximizing the likelihood is equivalent to minimizing the negative log likelihood, since the log function is strictly monotonically increasing.}.

From a Bayesian perspective, we define a prior distribution \(p(\bm\theta)\) and a likelihood function \(\lik\) such that the goal of optimization is to find the parameters \(\bm\theta^*\) which maximize the posterior distribution \(p(\bm\theta \given \bm y)\).
This way, we can quantify the uncertainty in our model parameters \(\bm\theta\) by computing the posterior distribution \(p(\bm\theta \given \bm y)\).

\subsection{Stochastic Gradient Descent}

Stochastic gradient descent (SGD) is a first-order optimization algorithm which iteratively updates the parameters \(\bm\theta\) in the direction of the negative gradient of the loss function \(\mathcal{L}(\bm\theta)\).
The update rule is given by
%
\begin{equation*}
    \bm\theta_{t+1} = \bm\theta_t - \eta_t \nabla_{\bm\theta} \mathcal{L}(\bm\theta_t)
\end{equation*}
%
where \(\eta_t\) is the learning rate at iteration \(t\).
The learning rate \(\eta_t\) can be constant or adaptive, and is often chosen to be a decreasing function of \(t\).
The gradient \(\nabla_{\bm\theta} \mathcal{L}(\bm\theta_t)\) is computed using a single training example \(\bm x_t, y_t\).
The gradient is computed using the chain rule as
%
\begin{equation*}
    \nabla_{\bm\theta} \mathcal{L}(\bm\theta_t) = \nabla_{\bm\theta} \log \lik(\bm x_t, y_t \given \bm\theta_t) \nabla_{\bm\theta} f_{\bm\theta_t}(\bm x_t)
\end{equation*}
%
where \(\nabla_{\bm\theta} f_{\bm\theta_t}(\bm x_t)\) is the gradient of the neural network with respect to its parameters \(\bm\theta_t\).

Note that \(\eta_t\) can be interpreted as the step size of the gradient descent algorithm.
It can either be constant throughout training (\(\eta_t = \eta\)) or adaptive (if it varies throughout training).

\section{The Hessian}

The Hessian is a matrix of second-order partial derivatives of a scalar function.
From now on, we will use the notation \(\nabla^2_{\bm\theta} f\) to represent the Hessian matrix of \(f\), where each element is given by \(\left(\nabla^2_{\bm\theta} f\right)_{i, j} = \frac{\partial^2 f}{\partial\theta_i \partial\theta_j}\).
Suppose we have a function \(\mathcal{L}: \reals^d \rightarrow \reals\) parameterized by \(\bm \theta \in \reals^d\).
Then the Hessian can be written as \(J_{\bm \theta}(\nabla_{\bm\theta} \mathcal{L})\).

We are interested in the Hessian of the loss function \(\mathcal{L}(\bm\theta)\) with respect to the parameters \(\bm\theta\).
For a neural network with \(d\) parameters, the Hessian is therefore a \(d \times d\) square matrix.
Furthermore, if all of the neural network's second partial derivatives are continuous, then the Hessian is symmetric and positive definite.
In this case, the Hessian is generally dominated by the block-diagonal \cite{martens2015optimizing}.

\section{The Generalized Gauss-Newton Approximation}

From the equation of the Hessian we can apply the chain rule twice and the product rule once to obtain a simpler expression
%
\begin{align}\label{eq:ggn-whole-hessian}
    \nabla^2_{\bm \theta} \mathcal{L}
    ={} & J_{\bm \theta}(\nabla_{\bm \theta} \mathcal{L})
    = J_{\bm \theta}\left(\nabla_f \mathcal{L} \cdot \nabla_{\bm \theta} f\right) \nonumber
    \\ ={}& J_{\bm \theta} (\nabla_{\bm \theta} f) \cdot \nabla_f \mathcal{L} + J_{\bm \theta} \left(\nabla_f \mathcal{L}\right) \cdot \nabla_{\bm \theta} f \nonumber
    \\ ={}& \nabla^2_{\bm \theta} f \cdot \nabla_f \mathcal{L} + \nabla_{\bm \theta} f \T \cdot \nabla^2_f \mathcal{L} \cdot \nabla_{\bm \theta} f
\end{align}
with \(\nabla_f\) representing the gradient with respect to \(f(\bm x)\).

Assume now we will approximate \(f\) using a first-order Taylor expansion.
This approximation is given by
%
\begin{align}\label{eq:nn-taylor}
    f_{\bm \theta}(\bm x) \approx{} f_{\bm \theta_0}(\bm x) + \nabla_{\bm \theta} f_{\bm \theta_0}(\bm x) \cdot (\bm \theta - \bm \theta_0) .
    \\ \text{Notice that } \nabla^2_{\bm \theta} f_{\bm \theta_0}(\bm x) = 0 \nonumber
\end{align}
%
% Notice that, in this approximation, we have \(\nabla^2_{\bm \theta} f_{\bm \theta_0}(\bm x) = 0\).
In this way, we are linearizing our neural network function.
By combining \cref{eq:ggn-whole-hessian} and \cref{eq:nn-taylor} we then obtain the Gauss-Newton Hessian approximation,
%
\begin{align*}
    \nabla^2_{\bm\theta} \mathcal{L}
    \approx{} & (\nabla_{\bm\theta} f)\T (\nabla^2_f \mathcal{L}) (\nabla_{\bm\theta} f)
\end{align*}

For mean square error (MSE) loss, we have one output variable (\(o = 1\)), so \(f: \reals^n \times \reals^d \rightarrow \reals\).
From the expression of MSE loss, we get the GGN-approximate Hessian
%
\begin{align*}%\label{eq:mse-hessian}
    \nabla^2_{f} \mathcal{L} = & {} \frac{1}{N} \sum_{n=1}^N (y_n - f(\bm x_n))^2 = 1 \nonumber
    \\ \Longrightarrow \nabla^2_{\bm\theta} \mathcal{L} =&{} (\nabla_{\bm\theta} f)\T (\nabla_{\bm\theta} f)
\end{align*}

\section{Fisher Information}\label{sec:fisher-information}
% https://agustinus.kristia.de/techblog/2018/03/11/fisher-information/

Consider the gradients of the log-likelihood optimization objective, given by the expression \(\nabla_{\bm\theta} \log\lik\).
We can show that the expectation of these gradients is zero:
\begin{align*}
    \E{\lik}{\nabla_{\bm\theta} \log\lik} = & {} \E{\lik}{\nabla \log \lik}
    \\ =&{} \int \nabla \log \lik \lik \, d \bm y
    \\ =&{} \int \frac{\nabla \lik}{\lik} \lik \, d \bm y % as per the chain rule
    \\ =&{} \int \nabla \lik \, d \bm y % as per the Leibniz integral rule
    \\ =&{} \nabla \int \lik \, d \bm y
    \\ =&{} \nabla 1
    \\ =&{} 0
\end{align*}

Let us now analyze the covariance of these gradients.
It is given by
%
\begin{align}\label{eq:gradient-covariance}
    %  & \cov{\lik}{\nabla \log\lik}
    \E{\lik}{\left(\nabla \log\lik - \E{}{\nabla \log\lik}\right) \left(\nabla \log\lik - \E{}{\nabla \log\lik}\right)\T} \nonumber
    \\ = \E{\lik}{\left(\nabla \log\lik - 0\right) \left(\nabla \log\lik - 0\right)\T} \nonumber
    \\ = \E{\lik}{\nabla \log\lik \nabla \log\lik\T} \nonumber
    \\ \equiv \mathcal{F}(\bm \theta)
\end{align}
%
where we have defined the Fisher information matrix \(\mathcal{F}(\bm \theta)\) as the expectation of the outer product of the score function under our model.
This product is usually intractable.
It can, however, be approximated empirically by using samples from an empirical distribution \(q(\bm y)\) which is close to the true likelihood \(\lik\) \cref{eq:empirical-fisher}.
%
\begin{equation}\label{eq:empirical-fisher}
    \mathcal{F} \approx \frac{1}{N} \sum^N_{n=1} \nabla \log p(y_n \given \bm \theta) \nabla \log p(y_n \given \bm \theta)\T.
\end{equation}
%
Note, however, that this matrix is distinct from the Fisher information matrix~\cite{martens2020new, kunstner2019limitations}, since

Another nontrivial property of the Fisher information matrix is that it is equivalent to the negative expectation of the Hessian of the log-likelihood.
%
\begin{align*}
    H_{\bm\theta} \left(\log \lik\right) = & {} \nabla^2_{\bm\theta} \log \lik
    \\ =&{} \nabla_{\bm\theta} \nabla_{\bm\theta} \log \lik
    \\ =&{} \nabla_{\bm\theta} \frac{\nabla_{\bm\theta} \lik}{\lik}
    \\ =&{} \frac{}{}
\end{align*}
%
The Fisher information matrix can therefore be interpreted as the curvature of the log-likelihood.

RELATIONSHIP WITH GAUSS-NEWTON APPROXIMATION
for common loss functions, the GGN is equivalent to the Fisher information matrix~\cite{martens2020new}

RELATIONSHIP WITH KL DIVERGENCE

\section{The Natural Gradient}
% https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/


% Natural Gradient Descent is an approximate second-order optimisation method. It has an interpretation as optimizing over a Riemannian manifold using an intrinsic distance metric, which implies the updates are invariant to transformations such as whitening. By using the positive semi-definite (PSD) Gauss-Newton matrix to approximate the (possibly negative definite) Hessian, NGD can often work better than exact second-order methods.

\cite{amari1998natural} introduced the natural gradient as a way to optimize a function \(f\) parameterized by \(\bm\theta\) by following the direction of steepest descent in the Fisher information metric.
It can be interpreted as
The algorithm can be seen in \cref{natural-gradient}.
%
\begin{algorithm}
    \caption{Natural Gradient Descent} \label{natural-gradient}
    \begin{algorithmic}[1]
        \For {$i = 1$ to $T$}
        \State Calculate \(\mathcal{L}(\bm\theta)\)
        \State Calculate the gradient of the loss \(\nabla_{\bm\theta} \mathcal{L}(\bm\theta)\)
        \State Calculate the Fisher information matrix \(\mathcal{F}(\bm\theta)\)
        \State Calculate the natural gradient \(\nabla_{\bm\theta} \mathcal{L}(\bm\theta) \mathcal{F}(\bm\theta)^{-1}\)
        \State Update the parameters \(\bm\theta \gets \bm\theta - \eta \nabla_{\bm\theta} \mathcal{L}(\bm\theta) \mathcal{F}(\bm\theta)^{-1}\)
        \EndFor
        \State \Return {\(\bm\theta\)}
    \end{algorithmic}
\end{algorithm}
%
In practice, deep learning models have millions (or billions) or parameters, and the Fisher information matrix is consequently often too large to be computed and stored.
This therefore limits the applicability of the natural gradient.
Instead, it is often approximated by the diagonal of the Hessian.

% This brings us to the Adam optimizer, which is a popular choice for optimizing deep neural networks.
\cite{kingma2014adam}
\cite{martens2015optimizing}
\cite{botev2017practical}

For a recent, detailed discussion of the natural gradient, see \cite{martens2020new}.
\cite{wu2019logan}

\section{Conjugate Gradient Descent}

If allocation of memory to store a matrix is infeasible, but the computational graph for this matrix can be determined, then the conjugate gradient method can be used to approximate the inverse of this matrix~\cite{giordano2018covariances, nocedal1999numerical}.

Instead of computing
\begin{align*}
    \bm \theta^* ={} & H^{-1} \bm \epsilon
    \\\Rightarrow H \bm \theta^* ={} & H H^{-1} \bm \epsilon
    \\\Rightarrow H \bm \theta^* ={}& \bm \epsilon.
\end{align*}

EXPLANATION
\emph{When even calculating or instantiating Hηη is prohibitively time-consuming, one can use conjugate gradient algorithms to approximately compute H−1 ηη gη (Wright and Nocedal, 1999, Chapter 5). The advantage of conjugate gradient algorithms is that they approximate H−1 ηη gη using only the Hessian-vector product Hηη gη, which can be computed efficiently using automatic differentiation without ever forming the full Hessian Hηη. See, for example, the hessian vector product method of the Python autograd package (Maclaurin et al., 2015). Note that a separate conjugate gradient problem must be solved for each column of gη| , so if the parameter of interest g (θ) is high-dimensional it may be faster to pay the price for computing and inverting the entire matrix Hηη . See 5.3.2 for more discussion of a specific example. In Theorem 2, we require η0∗ to be at a true local optimum. Otherwise the estimated sensitivities may not be reliable (e.g., the covariance implied by Eq. (14) may not be positive definite). We find that the classical MFVB coordinate ascent algorithms (Blei et al. (2016, Section 2.4)) and even quasi-second order methods, such as BFGS (e.g., Regier et al., 2015), may not actually find a local optimum unless run for a long time with very stringent convergence criteria. Consequently, we recommend fitting models using second-order Newton trust region methods. When the Hessian is slow to compute directly, as in Section 5, one can use the conjugate gradient trust region method of Wright and Nocedal (1999, Chapter 7), which takes advantage of fast automatic differentiation Hessian-vector products without forming or inverting the full Hessian.}
